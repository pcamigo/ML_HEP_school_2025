{"cells":[{"cell_type":"markdown","metadata":{"id":"TIb7AW-J_Q0j"},"source":["# Hands-on session Thursday 16\n","# Photometric redshift using Decision Trees, Random Forest and fully connected neural network\n","\n","This notebook is designed to guide you step by step through the development of the mini-project by following each cell. In some cases, **specific tasks are highlighted in bold** for you to complete. You can review the provided solutions to compare your results.\n","\n","\n","\n","### What is redshift?\n","\n","We now know that stars and galaxies are located at varying distances, sometimes billions of light-years away. Measuring these distances is crucial for creating a 3D map of the Universe and converting apparent brightness into intrinsic luminosity, revealing the true energy output of astrophysical sources. Additionally, because light travels at a finite speed, distance corresponds to lookback time, allowing us to study the Universe’s history.\n","\n","Moreover, Edwin Hubble discovered in 1929 that the universe is *expanding*, and the farther away a galaxy is, the faster it is moving away from us. This causes a Doppler effect in the light we receive from far objects. In astrophysics, this doppler shift is known as **redshift** $z$, and can be expressed in respect to the observed and emitted wavelength:\n","\n","$$ 1 + z =\\frac{\\lambda_{obs}}{\\lambda_{em}} $$\n","\n","The best way to determine redshifts is by calculating the shift of known lines in a *spectrum*, which is the light emission as a function of wavelength.  However, spectra might be low quality if the objects aret too far away, and is expensive to obtain high quality spectral data. Photometry, on the other hand, is much cheaper and easy to obtain. In this case, we have the average emission (or brightness) in a range of wavelengths (known as band, or filter).\n","\n","\n","\n","<img src=\"https://skyserver.sdss.org/dr14/en/get/SpecById.ashx?id=1833056915844786176\" alt=\"spectra\" width=\"500\">\n","\n","\n","\n","Therefore, we can avail ourselves of photometric data for much larger samples of galaxies in the Universe, and upcoming surveys are slated to image a significant fraction (>10%) of all the galaxies in the Universe. Being able to derive reliable redshifts for these galaxies is crucial for astronomy.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### The dataset\n","The dataset used in this notebook comes from this paper by [Zhou et. al (2019)](https://academic.oup.com/mnras/article/488/4/4565/5538813). It is a compilation from several surveys, such as DEEP2, DEEP3 and 3D-HST. The data resembles the wavelength coverage and depth of the upcoming Vera Rubin Observatory which is expected to provide photometry in six bands, ranging from near- ultraviolet to near-infrared (u, g, r, i, z, and y), of approximately 20 billion galaxies, spanning a considerable fraction of the Universe’s volume.\n","\n","\n","The results obtained in the paper are shown in the following figure\n","\n","\n","<img src=\"https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/mnras/488/4/10.1093_mnras_stz1866/1/stz1866fig5.jpeg?Expires=1739848857&Signature=A1JoKdBTtx6umURSd7FPeRWR19vwiRfeiOyK7WSoo-ZTE6-B5QXwy7HCyPmiUBykW8d1e60lQ4CFLvcHjinABBnbfl7xHMgMvx9gHGRIgISI6j-8bxfCaWGojXMhdU-ZkUxwhJ0OsbIGrYDCp1W~HECl10BOVZ3HPq7Qjv6D4BPYsSMX-JGqNoqW-lIxJ0HxWhxl63kPGvfwvdt~H1ZL7KaK2oDaPMhepxqRvr17P-~H7TW0j-lY-D940m5il-Ffq0Bs17Rf6sUDIj3qwwvvU~km~11W4Zrf2RYf~rDf1w3gXY2zAvYojYwdOIFo3Mo3wR6hg7RGuOrmrMDz8E7PAQ__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\" alt=\"photo_z_paper\" width=\"500\">\n","\n","\n","where  $\\sigma_{NMAD}$ is the normalized median absolute deviation of the residuals and $\\eta$ is the outlier fraction defined as the objects with redshifts such as $\\frac{|(z_{spec} - z)|}{(1+z_{spec})} > 0.15$.\n","\n"," In the paper, the values are $\\sigma_{NMAD}=0.0534$ and $\\eta=10.61\\%$ (outlier fraction)\n"],"metadata":{"id":"4vM_e8dczFw0"}},{"cell_type":"markdown","source":["**You can explore the data and check the available features in the data set. Check the columns and the number of objects present in the database**\n","\n"],"metadata":{"id":"ReP9B9tJ1YVH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RFzkWjVO1o-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy import stats\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, validation_curve\n","from sklearn.model_selection import KFold, cross_validate, cross_val_predict, GridSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.inspection import permutation_importance\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"jf7ioxIh1-UZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib\n","import matplotlib.pyplot as plt\n","\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_colwidth', 150)\n","\n","font = {'size'   : 10}\n","matplotlib.rc('font', **font)\n","matplotlib.rc('xtick', labelsize=8)\n","matplotlib.rc('ytick', labelsize=8)\n","matplotlib.rcParams['figure.dpi'] = 300"],"metadata":{"id":"LJUaDBJ432KC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install astropy\n","\n"],"metadata":{"id":"Co9SKmHb3M5L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import astropy\n","\n","from astropy.io import fits"],"metadata":{"id":"9pT0IVsZ3PTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with fits.open('/content/drive/MyDrive/escuela_ML_USM_2025/DEEP2_uniq_Terapix_Subaru_v1.fits') as data:\n","    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder())\n","    #https://numpy.org/doc/2.1/user/byteswapping.html"],"metadata":{"id":"2mnLsvMu3Vxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#explore the data"],"metadata":{"id":"IY8oYr4R3Vt2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**From the features, we need to select only the brightness of the galaxies, in the filters of interest ('u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor').**\n","\n","<img src=\"https://noirlab.edu/science/sites/default/files/media/archives/images/filters_u_DR1.png\" alt=\"ugrizY\" width=\"500\">\n","\n","\n","**Filter the dataframe to only have those features as columns in a dataframe named `features`**"],"metadata":{"id":"uM-zludd3yo9"}},{"cell_type":"markdown","source":["**The variable we want to predict (target) is the redshift, `zhelio`; define an array or Series `target` with the values of redshift**"],"metadata":{"id":"_ERyCuZ64LGL"}},{"cell_type":"markdown","source":["### Part 1: Decision Trees\n","\n","First we will implement a simple decision tree (regressor) to predict the redshift. You need to split the data into train and test, select the ratio as you wish (always leave the bigger fraction for training!)\n"],"metadata":{"id":"hPzhEVmq45uU"}},{"cell_type":"code","source":["#split te data into train/validation/test\n","\n","indices = np.arange(len(target))\n","\n","# first reserve 70% of the data for training, 30% for validation\n","X_train, X_validate, y_train, y_validate, indices_train, indices_validate = train_test_split(features, target, indices, test_size=0.3, random_state=42)\n","\n","# second, split the validation set in half to obtain validation and test sets.\n","X_validate, X_test, y_validate, y_test, indices_validate, indices_test = train_test_split(X_validate, y_validate, indices_validate, test_size=0.5, random_state=42)"],"metadata":{"id":"w1qrHsXB3k1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(X_validate.shape)\n","print(X_test.shape)"],"metadata":{"id":"MfrY33Jd3ktW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_test.shape)\n","print(y_validate.shape)\n","print(y_train.shape)"],"metadata":{"id":"gUDQFTLdDLWv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","A decision tree is composed of a series of if-else decision steps. The number of steps and the types of decision at each step is determined by training the algorithm with supervision.\n","Call DecisionTreeRegressor (with the default parameteres....for now) from sklearn and fit the tree to your training data. Then, obtain the predictions in your test sets"],"metadata":{"id":"agWbJOK035lj"}},{"cell_type":"code","source":["# Initialize the model\n","dtree = DecisionTreeRegressor()\n","\n","# Fit the model parameters using the training dataset\n","dtree.fit(X_train, y_train)"],"metadata":{"id":"2wD1-PwH3kho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_predict = dtree.predict(X_test)\n"],"metadata":{"id":"g7QMXtTq7sNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_test.shape)"],"metadata":{"id":"M9SF3spRDPE8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Since this is a Regression problem, obtain a proper metric to evaluate this algorithm (MAE, MAPE, MSE, etc), and calculate the NMAD and outlier fraction. Also, plot the distribution of redshifts (in the range 0-3) for the predicted values and the true (test) values**"],"metadata":{"id":"ySzex5Vm7ya_"}},{"cell_type":"code","source":["print(f'MSE = {mean_squared_error(y_test, y_predict):.4f}')"],"metadata":{"id":"X1U0Ihgq7xLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(y_test,bins=50,density=False,alpha=0.5, range = (0,3), label = 'True');\n","plt.hist(y_predict,bins=50,density=False,alpha=0.5, range = (0,3), color = 'g', label = 'Predicted');\n","plt.legend(fontsize=14);"],"metadata":{"id":"wUB3HZY1-tfR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**To calculate NMAD and outlier fraction, maybe you need to flatten the arrays, see line below:**"],"metadata":{"id":"NQBtSxxh_dYW"}},{"cell_type":"code","source":["y_test = np.array(y_test).flatten()  # Flatten if needed\n","y_predict = np.array(y_predict).flatten()  # Flatten if needed"],"metadata":{"id":"mJdEFwBmC37w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_test.size)\n","print(y_predict.size)"],"metadata":{"id":"wBeQ9lpKBsaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["1.48*np.median(np.abs(y_test-y_predict)/(1 + y_test)) #NMAD\n"],"metadata":{"id":"oVqCzYXA981a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rQDTrFZqAATY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(np.where(np.abs(y_test-y_predict)>0.15*(1+y_test))[0])/len(y_test) #outlier fraction"],"metadata":{"id":"8r6fzTiV8PWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**How does your result compares with the results in the paper?**"],"metadata":{"id":"GJ9yul6BAzXx"}},{"cell_type":"code","source":[],"metadata":{"id":"0j67xXiDCK9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Make a plot of Truth (test) and Prediction to check how they compare**"],"metadata":{"id":"sta9C4jyBVPL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCi2a2GB_Q0m"},"outputs":[],"source":["#plot here\n"]},{"cell_type":"markdown","source":["**What do you think of your results?**"],"metadata":{"id":"jyQZD_88BkWK"}},{"cell_type":"markdown","source":["Not so good!\n"],"metadata":{"id":"pICtpbu3Y_ec"}},{"cell_type":"code","source":[],"metadata":{"id":"ABZDLHWXBq6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ty1aqrju_Q0m"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["One way to improve the results of a model is to tune its parameters to run appropiately. In decision trees, this is particularly important, since an unleashed tree will try to fit all the train data, which can lead to overfitting.\n","\n","Below is an implementation of [`RandomizedSearchCV`](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), which will randomly samples a specified number of parameter combinations from a given range and evaluates them using cross-validation to find the best-performing combination.\n","The advantage of this method over other optimization (such as [`GridSearchCV`](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html) is the computing time, which might be too long for larger datasets)"],"metadata":{"id":"9JeTkgHkBriz"}},{"cell_type":"code","source":["for key in DecisionTreeRegressor().get_params().keys():\n","    print(key)"],"metadata":{"id":"lpYiEuuWDd4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hyperparameter_distributions = {\n","    'max_depth': np.arange(1, 20, 2).astype(int),\n","    'min_samples_split': np.arange(5, 105, 10).astype(int),\n","    'min_samples_leaf': np.arange(5, 105, 10).astype(int)\n","}\n","\n","random_search = RandomizedSearchCV(\n","    dtree,\n","    param_distributions=hyperparameter_distributions,\n","    n_iter=100\n",")\n","\n","random_search.fit(X_train, y_train.values.ravel())"],"metadata":{"id":"Wa5wyUHWDdx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The best results are"],"metadata":{"id":"vhoaIjnWbMOR"}},{"cell_type":"code","source":["print(random_search.best_params_)\n"],"metadata":{"id":"OrsmMc2eDdqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Now we can check whether the optimization of hyperparameters had any effect in the results. Plot again the comparison between test and prediction, and calculate the metrics and $\\sigma_{NMAD}$ and the outlier fraction**"],"metadata":{"id":"LrMi7qFjELRq"}},{"cell_type":"code","source":[],"metadata":{"id":"eXW9cJy-EK5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot here"],"metadata":{"id":"6vuH_ZcEEpjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["1.48*np.median(np.abs(y_test-y_predict)/(1 + y_test))\n"],"metadata":{"id":"3P53BVCZEper"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(np.where(np.abs(y_test-y_predict)>0.15*(1+y_test))[0])/len(y_test)"],"metadata":{"id":"YoXLfshJEpWI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Do you think there was an improvement?**"],"metadata":{"id":"-6txA4ViE-Mj"}},{"cell_type":"code","source":[],"metadata":{"id":"WnYFzyT_FIQF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part 2 Random Forest\n","It is clear that Decision Trees are not very good predictors of redshift. We obtain a high fraction of outliers and certainly we can do better with MSE. So, to improve the metrics, we'll try a more complex approach, with **Random Forests**, which are tree-based supervised learning models."],"metadata":{"id":"n3bU1NHUFG7G"}},{"cell_type":"markdown","source":["We initialize the model [`RandomForestRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"],"metadata":{"id":"ne2hdLo9WuCa"}},{"cell_type":"code","source":["model = RandomForestRegressor()"],"metadata":{"id":"r4QsS6R16X6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we can check the parameters of the model...notice that some of those are for each tree and others, for the forest (ensemble)"],"metadata":{"id":"Kn6qceypXA6q"}},{"cell_type":"code","source":["model.get_params()"],"metadata":{"id":"y0zVlGA56X3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will use cross validation to obtain the metrics with training and test data, and to get some diagnostics of the results. Below is the implementation, check the parameters of KFold and try to change it and see what you obtain. More information about cross validation, KFold and other methods in this [link](https://scikit-learn.org/1.5/modules/cross_validation.html).\n","\n","The results are the metric scores (as default, it calculates $R^2$, but you can change this in Kfold with the `scoring` parameter of cross_validate (see sklearn documentation)"],"metadata":{"id":"g1-4YySDXvTy"}},{"cell_type":"code","source":["%%time\n","#it will take about 2 minutes\n","scores = cross_validate(model,features,target, cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score=True)"],"metadata":{"id":"SLKfjXdI6X1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores"],"metadata":{"id":"Dnt-25s76Xy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.mean(scores['test_score'])"],"metadata":{"id":"dQLX4gwE6Xvy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.mean(scores['train_score'])"],"metadata":{"id":"8yF0aK4m6Xm6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**How does the train score compares with the test score? What can you say about that?**"],"metadata":{"id":"AyxIGYuDcuwr"}},{"cell_type":"markdown","source":["Let's compare the true values with the predictions. When we run `cross_validate()` we only obtain the metric scores, if we want the predictions, we use `cross_val_predict()`"],"metadata":{"id":"1qcRhr-1c6qQ"}},{"cell_type":"code","source":["ypred = cross_val_predict(model,features,target, cv = KFold(n_splits=5, shuffle=True, random_state=10))"],"metadata":{"id":"hHfOeNo66Xd2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Plot the comparison between prediction and true values for random forests**"],"metadata":{"id":"DIJDQdQ3doRr"}},{"cell_type":"code","source":["#plot here"],"metadata":{"id":"TMvB0eOM7joq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Also, check and compare the distribution of true and predicted redshifts (histogram)**"],"metadata":{"id":"KdYB5h9OdxTk"}},{"cell_type":"code","source":["#plot here"],"metadata":{"id":"U2SUwRcC7jmB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Calculate the $\\sigma_{NMAD}$ and outliers fraction and again, compare with the paper...**"],"metadata":{"id":"ykBHYI-9hO4d"}},{"cell_type":"code","source":[],"metadata":{"id":"wG_thp9M7jji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sU_XjV9u7jg5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You probably notice that we are not doing so good. We have a **high variance**, then we are **overfitting**. This is typical for trees and forest, when are \"unleashed\". We can try to optimize the hyperparameters of the model. To save some time, we will select a random subsample of the data and optimize with it."],"metadata":{"id":"TrYui4nXhcFF"}},{"cell_type":"markdown","source":["**Select a random subsample of the data (features and target)**"],"metadata":{"id":"sJP19xwwh3pH"}},{"cell_type":"code","source":["#code here"],"metadata":{"id":"HhAsxVGL7jeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1sTAQxDi7jbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lFfW7KUN7jY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**It is good practice to ensure that the performance on the smaller set remains similar to the one obtained on the entire data set, which means that the change in size will not significantly affect the optimization process. Check the metrics with cross_validate and compare with the one obtained previously with random forest (our benchmark)**"],"metadata":{"id":"pwVY15CNiJa8"}},{"cell_type":"code","source":["#code here"],"metadata":{"id":"10G2LSSi7jV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bPt81bM_7jSN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**If the metrics are similar as before, we can move on to the optimization of hyperparameters. As in the case of Decision trees, perform a `RandomizedSearchCV(). Check also the hyperparameters of RF, and decide which ones you want to use. More information of the parameters you can find [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)**\n","\n","\n","As as suggestion, use this grid\n","\n","`\n","parameters = {\n","    'min_impurity_decrease': [0.1, 0.5, 0.0],\n","    'max_features': [None, 4, 2],\n","    'n_estimators': [50, 100, 200],\n","    'min_samples_split': [10, 20, 100],\n","    'max_leaf_nodes': [None, 100, 200]\n","}`\n"],"metadata":{"id":"p3qqq_5dilBh"}},{"cell_type":"code","source":["model.get_params()"],"metadata":{"id":"Gu-_VFuz9jBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#code here"],"metadata":{"id":"Je0HXTGyCN3D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can create a dataframe with the scores for each combination of parameters, sorted from the best models to the worst"],"metadata":{"id":"XWAwvbzVkjCa"}},{"cell_type":"code","source":["scores = pd.DataFrame(model.cv_results_)\n","scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n","                                                    ascending = False)\n"],"metadata":{"id":"7BafgiIe9i8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scoresCV"],"metadata":{"id":"yihn3nnb9i6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Did we obtain better results?\n","\n","Probably you already notice that we have not improve the results. :("],"metadata":{"id":"8qeZh4dPk02J"}},{"cell_type":"markdown","source":["### Part 3: A little of Feature engineering\n","\n","Before giving up, and after noting that our issue is so severe that it's hard to attribute it to an optimization or chooice of algorithm issue, we need to look more carefully at data cleaning and/or imputing (something, in fact, that should be the first step in our pipeline!)\n"],"metadata":{"id":"yonNpE3Sk_Bo"}},{"cell_type":"markdown","source":["If you check the [paper](https://arxiv.org/pdf/1903.08174) (Table 6) you will notice that the authors consider many more columns that the ones we are using here. There are in particular some columns with information about the *quality* of the data provided. These columns are\n","- 'subaru_source':  source of the Y -band photometry: 0 = deep pointing; 1 = shallow pointing; -99 = not observed\n","- 'cfhtls_source': source of the ugri(i2)z photometry: 0 = Deep field; 1 = Wide field; -99 = not observed\n","- 'zquality':  (In DEEP2/3 catalog) DEEP2/3 redshift quality flag"],"metadata":{"id":"NC7_ZIs0lvuw"}},{"cell_type":"markdown","source":["**Create a dataframe with the columns we selected before and the three new ones mentioned above**"],"metadata":{"id":"cbWYcTbymdQc"}},{"cell_type":"code","source":["#code here"],"metadata":{"id":"-9qLtZxS9i4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3epXyaVY9i2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yPugbrYW9i0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**So, we need to filter the data based on quality. Filter your data with these criterias:**\n","\n","- **redshift quality: only use objects with high-quality spectroscopic redshift measurements 'zquality' >= 3**\n","- **select objects with cfhtls deep photometric data ('cfhtls_source' == 0)**\n","- **select objects with subaru deep photometry ('subaru_source' == 0)**\n","- **Also, unavailable measurements are marked by -99 or 99 (while typical values are around 20-25). We also get rid of data with missing measurements.**\n"],"metadata":{"id":"0aurlhSunFxH"}},{"cell_type":"code","source":["#code here\n"],"metadata":{"id":"6uMutmlS9iyn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bR1bIjpp9ivd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q76uAUEn9isw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C31cTx7QDafQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KOn_eBGGDacc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KrXl0HdyDaZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K1AmQF4tDaWc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Of course, remember to select from the target array the corresponding rows from your selections above**"],"metadata":{"id":"H5qemRpEoCcu"}},{"cell_type":"code","source":["#code here"],"metadata":{"id":"trkp7DdHDaTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gR_1hirRoMS4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Now, repeat the cross_validate() process with RandomForestRegressor (you can copy the code above) but using the selected data and obtain the metrics**"],"metadata":{"id":"xsClj8LfoNH3"}},{"cell_type":"code","source":["#code here"],"metadata":{"id":"I4fNX8KUDxhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jeJUF8BMDxfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"niLbuuFqDxct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**You can try to optimized the hyperparameters again to improve the results**"],"metadata":{"id":"lac-6Li9otPq"}},{"cell_type":"code","source":["#code here"],"metadata":{"id":"PiJtXI4kDxZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#code here"],"metadata":{"id":"V5bRIDUNDxXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aohvUOx6FCgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fZcv_CzTFCWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FrR2bQjiFCTi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Re plot the comparison between true and predicted values and calculate the outlier fraction and $\\sigma_{NMAD}$ and compare them to the paper**"],"metadata":{"id":"ixN8IGwypqyE"}},{"cell_type":"code","source":["#plot here"],"metadata":{"id":"kj5fS9TTFCQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xxN0_98lFRhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7sJZbcmSFRdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8hXUEDiLFRWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**So, what do you think? did it improve the result??**"],"metadata":{"id":"tIaZbY3Cp5OU"}},{"cell_type":"code","source":[],"metadata":{"id":"En4SQ1ShFRNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In any case, the lesson here is that with a simple improvement in the data (selecting by data quality) we improve much more the model than only optimizing the hyperparameters. That's the reason why is so important to explore and understand the data!**"],"metadata":{"id":"gAhuOXjKqH3V"}},{"cell_type":"code","source":[],"metadata":{"id":"vhIl4v_tqXOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part 4: Let's go neural!"],"metadata":{"id":"CYc0XEPvqX9G"}},{"cell_type":"markdown","source":["We now will use  a fully connected neural network to solve the problem. With RF we already obtain a very good result, but let's see if we can improve it substantially with a more complex sophisticated model, such as NN"],"metadata":{"id":"mCc6cdBMqg5H"}},{"cell_type":"code","source":[],"metadata":{"id":"AMcQltECqXt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAX7Ddfd_Q0n"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzdIblJi_Q0n"},"outputs":[],"source":["import keras\n","\n","from keras.models import Sequential #the model is built adding layers one after the other\n","\n","from keras.layers import Dense #fully connected layers: every output talks to every input\n","\n","from keras.layers import Dropout #for regularization"]},{"cell_type":"markdown","source":["WE will make a copy of the sel_features and sel_targets to name them X and y respectively. We will use the better quality selection of the whole data"],"metadata":{"id":"ACMXkDsnq7CL"}},{"cell_type":"code","source":["X=sel_features.copy()\n","y=sel_target.copy()"],"metadata":{"id":"JfOpYfw_F4GF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will split the data in train/validation/test. First we will shuffle them"],"metadata":{"id":"ekR3Ow9Brnh9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"100MCfNg_Q05"},"outputs":[],"source":["X,y = shuffle(X,y, random_state = 12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JEkymDS_Q05"},"outputs":[],"source":["fifth = int(len(y)/5) #Divide data in fifths to use 60/20/20 split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"II0EN1pV_Q05"},"outputs":[],"source":["X_train = X.values[:3*fifth,:]\n","y_train = y[:3*fifth]\n","\n","X_val = X.values[3*fifth:4*fifth,:]\n","y_val = y[3*fifth:4*fifth]\n","\n","X_test = X.values[4*fifth:,:]\n","y_test = y[4*fifth:]"]},{"cell_type":"markdown","metadata":{"id":"4aHqfJPv_Q05"},"source":[" we need to scale our data! NNs use matrix operations with weights and biases to compute activations. These involve multiplying input features by weights and passing them through activation functions like ReLU or sigmoid. Large values of the features may affect these operations, affect the convergence of the NN and avoid a bias towards features with highr values\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGcMKwtu_Q05"},"outputs":[],"source":["scaler = StandardScaler()\n","\n","scaler.fit(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaMj1-O0_Q06"},"outputs":[],"source":["Xst_train = scaler.transform(X_train)\n","Xst_val = scaler.transform(X_val)\n","Xst_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"r1zYQ1yg_Q06"},"source":["**In a regression problem, we will choose a different activation for the output layer (e.g. linear), and an appropriate loss function (MSE, MAE, ...).**\n","\n","**Our input layer has six neurons for this problem.**\n","\n","**For other parameters and the network structure, we can start with two layers with 100 neurons and go from there.**\n","\n","**Try to implement the NN (Sequential) with the architecture described. if you get loss, see the solution ;). As optimizer use Adam, and the activation function for intermediate layers, use RELU**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t37u_TiV_Q06"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C_pO8Jjl_Q06"},"source":["**Train the NN with  100 epochs and batch size = 300.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpC58myL_Q06"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9F0vSIG_Q07"},"outputs":[],"source":["results = model.evaluate(Xst_test, y_test)\n","print('MSE:', results) #we are only monitoring the MSE"]},{"cell_type":"markdown","metadata":{"id":"6o1--T2H_Q07"},"source":["**Plot the loss throughout the training process.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igJTaSwy_Q07"},"outputs":[],"source":["#plot here"]},{"cell_type":"markdown","metadata":{"id":"Npn26F0Uo0X3"},"source":["**As always with regression problems, it is helpful to plot the predictions against the true values.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4j5LEBae_Q07"},"outputs":[],"source":["#plot here\n"]},{"cell_type":"markdown","metadata":{"id":"AISbbpfKo-nn"},"source":["**We didn't do cross validation, so generate prediction on our single test fold in order to derive the other metrics we are interested in (OLF and NMAD).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB8PgXsG_Q08"},"outputs":[],"source":["#code here"]},{"cell_type":"markdown","metadata":{"id":"KMxHlNxDTD-h"},"source":["**Calculate Outlier Fraction**"]},{"cell_type":"code","source":[],"metadata":{"id":"Y_JIXybMHK0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dz_2Oq5c_Q08","scrolled":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3RYDUSKL_Q08"},"source":["**Calculate Normalized Median Absolute Deviation (NMAD)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvPLrYvB_Q08"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"K7rslnmK_Q08"},"source":["**So What was best? RF o NNs?**"]},{"cell_type":"markdown","source":["**Ok, this was a long exercise... What are your conclusions ??**"],"metadata":{"id":"hAGd6rlqADBi"}},{"cell_type":"markdown","source":["### Appendix: Optimization of parameters with Keras (optional)\n","\n","Below is implemented optimization with Keras tuner. It takes a long time! you can follow the cells to see hoy it works ;)"],"metadata":{"id":"W5WIKKQwAK37"}},{"cell_type":"markdown","metadata":{"id":"6fV8VkFC_Q08"},"source":["### Let's try some optimization with keras tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"veJnCwk8_Q08"},"outputs":[],"source":[" !pip3 install keras-tuner --upgrade    #You may have to install keras tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvw7d9oK_Q09"},"outputs":[],"source":["from keras_tuner.tuners import RandomSearch\n","from tensorflow.keras import layers\n","\n","#Some material below is adapted from the Keras Tuner documentation\n","\n","# https://keras-team.github.io/keras-tuner/"]},{"cell_type":"markdown","metadata":{"id":"JXOaqAvM_Q09"},"source":["This function specifies which parameters we want to tune. Tunable parameters can be of type \"Choice\" (we specify a set), Int, Boolean, or Float."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XgKwB1J_Q09"},"outputs":[],"source":["def build_model(hp):\n","    model = keras.Sequential()\n","    for i in range(hp.Int('num_layers', 2, 6)): #We try between 2 and 6 layers\n","        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n","                                            min_value=100, #Each of them has 100-300 neurons, in intervals of 100\n","                                            max_value=300,\n","                                            step=100),\n","                               activation='relu'))\n","    model.add(Dense(1, activation='linear')) #last one\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(\n","            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), #And a few learning rates\n","        loss='mse')\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"GmZr4Dgh_Q09"},"source":["Next, we specify how we want to explore the parameter space. The Random Search is the simplest choice, but often quite effective; alternatives are Hyperband (optimized Random Search where a larger fraction of models is trained for a smaller number of epochs, but only the most promising ones survive), or Bayesian Optimization, which attempts to build a probabilistic interpretation of the model scores (the posterior probability of obtaining score x, given the values of hyperparameters)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKTmAfeb_Q09"},"outputs":[],"source":["tf.keras.backend.clear_session()\n","\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_loss',\n","    max_trials=40, #number of combinations to try\n","    executions_per_trial=3,\n","    project_name='My Drive/Photoz') #may need to delete or reset"]},{"cell_type":"markdown","metadata":{"id":"IfwvvlOE_Q09"},"source":["We can visualize the search space below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPJiiBOD_Q09"},"outputs":[],"source":["tuner.search_space_summary()"]},{"cell_type":"markdown","metadata":{"id":"kU7aZ8CL_Q0-"},"source":["Finally, it's time to put our tuner to work. (This is a big job!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrqAgLjg_Q0-"},"outputs":[],"source":["tuner.search(Xst_train, y_train, #same signature as model.fit\n","             epochs=100, validation_data=(Xst_val, y_val), batch_size=300, verbose = 1)\n","\n","#Note: setting verbosity to 0 would give no output until done - it took about ~35 mins on my laptop"]},{"cell_type":"markdown","metadata":{"id":"7FlfCn9m_Q0-"},"source":["The \"results\\_summary(n)\" function gives us access to the n best models. It's useful to look at a few because often the differences are minimal, and a smaller model might be preferable! Note that the \"number of units\" parameter would have a value assigned to it for each layers (even if the number of layers is smaller in that particular realization)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AeqNold_Q0-"},"outputs":[],"source":["tuner.results_summary(6)"]},{"cell_type":"markdown","metadata":{"id":"kOE3vVSJ_Q0-"},"source":["The losses of the first few models are very similar, suggesting that 1. as usual, we need to do some form of cross-validation to be able to come up with a ranking, and 2. With 3-5 layers and a few hundred neurons per layer, the exact configuration doesn't matter too much."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dn06Jq-T_Q0-"},"outputs":[],"source":["best_hps = tuner.get_best_hyperparameters()[0] #choose first model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qAU0mn1_Q0-"},"outputs":[],"source":["best_hps.get('learning_rate')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cLPW3MH_Q0-"},"outputs":[],"source":["best_hps.get('num_layers')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b_LsMsr_Q0-"},"outputs":[],"source":["#Size of layers\n","\n","print(best_hps.get('units_0'))\n","print(best_hps.get('units_1'))\n","print(best_hps.get('units_2'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bq8ULBCb_Q0_"},"outputs":[],"source":["model = tuner.hypermodel.build(best_hps) #define model = best model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vilFKjD4_Q0_"},"outputs":[],"source":["model.build(input_shape=(None,6)) #build best model (if not fit yet, this will give access to summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nbegil5_Q0_"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"mfhllAIK3kgn"},"source":["Now, build a neural net with the optimal hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_gLSfuK_Q0_"},"outputs":[],"source":["bestnet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=300)"]},{"cell_type":"markdown","metadata":{"id":"3P5FN4i44ANi"},"source":["We can also look at the train vs validation curves for the optimal model found by the tuner."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezQT_8iw_Q0_"},"outputs":[],"source":["plt.plot(bestnet.history['loss'], label = 'train')\n","plt.plot(bestnet.history['val_loss'],'-.m', label = 'validation')\n","plt.ylabel('Loss', fontsize = 14)\n","plt.xlabel('Epoch', fontsize = 14)\n","plt.ylim(0,0.1)\n","plt.legend(loc='upper right', fontsize = 12)\n","plt.legend(fontsize = 12);\n","#plt.savefig('OptimalNN_Photoz.png',dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"D_nGIXfd4HlF"},"source":["Finally, we report test scores for all the metrics of interest (MSE, OLF, NMAD):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_C9_ECTO_Q0_","scrolled":true},"outputs":[],"source":["model.evaluate(Xst_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbvdcdQs_Q1A"},"outputs":[],"source":["ypred = model.predict(Xst_test)\n","\n","#Calculate OLF\n","\n","print('OLF', len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test))\n","\n","#Calculate Normalized Median Absolute Deviation (NMAD)\n","\n","print('NMAD', 1.48*np.median(np.abs(y_test-ypred)/(1 + y_test)))"]},{"cell_type":"markdown","metadata":{"id":"JKO11my0_Q1A"},"source":["These numbers have somewhat improved, compared to the baseline model; note that whether improvement is significant should be determined via cross validation."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"17Nu0Jcdlps_AxPsJ432dgerQHM5U0b2j","timestamp":1737045977010},{"file_id":"1XYmQ_AcAfsCxNU7H9x5qhwXVjBJt5Le-","timestamp":1736909072497},{"file_id":"1htVV3SYs3xjHGyI40b4Geu1x1VmTnvtb","timestamp":1736820734389}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}